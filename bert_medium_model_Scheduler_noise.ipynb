{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b912632f-5efc-40c0-ab96-1b0f83444f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/sdiwaka/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from qa_medium_model_scheduler.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 17841/17841 [02:59<00:00, 99.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER Score: 3.2254090967658806\n",
      "F1 Score: 0.4124168308886098\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from evaluate import load\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def extract_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    passage_list = []\n",
    "    query_list = []\n",
    "    response_list = []\n",
    "    for topic in data['data']:\n",
    "        for paragraph in topic['paragraphs']:\n",
    "            passage = paragraph['context'].lower()\n",
    "            for qa in paragraph['qas']:\n",
    "                query = qa['question'].lower()\n",
    "                for answer in qa['answers']:\n",
    "                    answer_text = answer['text'].lower()\n",
    "                    passage_list.append(passage)\n",
    "                    query_list.append(query)\n",
    "                    response_list.append({\n",
    "                        'text': answer_text,\n",
    "                        'answer_start': answer['answer_start'],\n",
    "                        'answer_end': answer['answer_start'] + len(answer_text)\n",
    "                    })\n",
    "    return passage_list, query_list, response_list\n",
    "\n",
    "class SQAD(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][i]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][i]),\n",
    "            'start_positions': torch.tensor(self.encodings['start_positions'][i]),\n",
    "            'end_positions': torch.tensor(self.encodings['end_positions'][i])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "def add_start_and_end_positions(queries, contexts, answers, tokenizer, max_length, doc_stride = 128):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    encodings = tokenizer(queries, contexts, max_length = MAX_LENGTH,truncation=True,padding=True, return_offsets_mapping=False, stride = doc_stride)\n",
    "    for idx, answer in enumerate(answers):\n",
    "        ret_start = 0\n",
    "        ret_end = 0\n",
    "        answer_encoding_fast = tokenizer(answer['text'], max_length=max_length, truncation=True, padding=True)\n",
    "        for a in range(len(encodings['input_ids'][idx]) - len(answer_encoding_fast['input_ids']) + 1):\n",
    "            match = True\n",
    "            for i in range(1, len(answer_encoding_fast['input_ids']) - 1):\n",
    "                if answer_encoding_fast['input_ids'][i] != encodings['input_ids'][idx][a + i]:\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:\n",
    "                ret_start = a + 1\n",
    "                ret_end = a + i + 1\n",
    "                break\n",
    "        start_positions.append(ret_start)\n",
    "        end_positions.append(ret_end)\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    return encodings\n",
    "\n",
    "def normalize_text(text):\n",
    "    def remove_articles(txt):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', txt)\n",
    "\n",
    "    def white_space_fix(txt):\n",
    "        return ' '.join(txt.split())\n",
    "\n",
    "    def remove_punc(txt):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in txt if ch not in exclude)\n",
    "\n",
    "    def lower(txt):\n",
    "        return txt.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(text))))\n",
    "\n",
    "class QuestionAnsweringModel(nn.Module):\n",
    "    def __init__(self, bert_base_model, device):\n",
    "        super(QuestionAnsweringModel, self).__init__()\n",
    "        self.bert = bert_base_model\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "        return start_logits, end_logits\n",
    "\n",
    "    def train_epoch(self, dataloader, optimizer, scheduler):\n",
    "        self.train()\n",
    "        loss_values = []\n",
    "        accuracy_values = []\n",
    "        for batch in tqdm(dataloader, desc='Training'):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            start_positions = batch['start_positions'].to(self.device)\n",
    "            end_positions = batch['end_positions'].to(self.device)\n",
    "            output_start, output_end = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.focal_loss(output_start, output_end, start_positions, end_positions, 1) #using gamma = 1\n",
    "            loss_values.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            start_predictions = torch.argmax(output_start, dim=1)\n",
    "            end_predictions = torch.argmax(output_end, dim=1)\n",
    "\n",
    "            accuracy_values.append(((start_predictions == start_positions).sum()/len(start_predictions)).item())\n",
    "            accuracy_values.append(((end_predictions == end_positions).sum()/len(end_predictions)).item())\n",
    "        scheduler.step()\n",
    "        return sum(accuracy_values)/len(accuracy_values), sum(loss_values)/len(loss_values)\n",
    "    \n",
    "    def focal_loss(self, start_logits, end_logits, start_positions, end_positions, gamma):\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        start_probs = softmax(start_logits)\n",
    "        inv_start_probs = 1 - start_probs\n",
    "        end_probs = softmax(end_logits)\n",
    "        inv_end_probs = 1 - end_probs\n",
    "        \n",
    "        log_softmax = nn.LogSoftmax(dim=1)\n",
    "        log_start_probs = log_softmax(start_logits)\n",
    "        log_end_probs = log_softmax(end_logits)\n",
    "        \n",
    "        negative_log_likelihood = nn.NLLLoss()\n",
    "        \n",
    "        start_loss = negative_log_likelihood(torch.pow(inv_start_probs, gamma) * log_start_probs, start_positions)\n",
    "        end_loss = negative_log_likelihood(torch.pow(inv_end_probs, gamma) * log_end_probs, end_positions)\n",
    "        \n",
    "        return (start_loss + end_loss) / 2\n",
    "    \n",
    "    def evaluate_model(self, dataloader, tokenizer):\n",
    "        self.eval()\n",
    "        f1_scores = []\n",
    "        answer_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                start_true_positions = batch['start_positions'].to(self.device)\n",
    "                end_true_positions = batch['end_positions'].to(self.device)\n",
    "\n",
    "                output_start, output_end = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                start_pred = torch.argmax(output_start, dim=1)\n",
    "                end_pred = torch.argmax(output_end, dim=1)\n",
    "\n",
    "                for i in range(input_ids.size(0)):\n",
    "                    pred_answer = tokenizer.decode(input_ids[i][start_pred[i]:end_pred[i]+1])\n",
    "                    true_answer = tokenizer.decode(input_ids[i][start_true_positions[i]:end_true_positions[i]+1])\n",
    "                    answer_list.append([pred_answer, true_answer])\n",
    "                    f1_scores.append(compute_f1_score(pred_answer, true_answer))\n",
    "        pred_answers = [ans[0] if ans[0] else \"$\" for ans in answer_list]\n",
    "        true_answers = [ans[1] if ans[1] else \"$\" for ans in answer_list]\n",
    "        wer_score = wer_metric.compute(predictions=pred_answers, references=true_answers)\n",
    "        avg_f1_score = sum(f1_scores) / len(f1_scores) if f1_scores else 0\n",
    "        return avg_f1_score, wer_score\n",
    "    \n",
    "def compute_f1_score(pred_answer, true_answer):\n",
    "    pred_tokens = normalize_text(pred_answer).split()\n",
    "    true_tokens = normalize_text(true_answer).split()\n",
    "    common_tokens = Counter(pred_tokens) & Counter(true_tokens)\n",
    "    num_common = sum(common_tokens.values())\n",
    "    if num_common == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_common / len(pred_tokens)\n",
    "    recall = 1.0 * num_common / len(true_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "train_data = 'spoken_train-v1.1.json'\n",
    "test_data = 'spoken_test-v1.1_WER44.json'\n",
    "train_passages, train_queries, train_response = extract_data(train_data)\n",
    "test_passages, test_queries, test_response = extract_data(test_data)\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "MODEL_PATH = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "train_encodings = add_start_and_end_positions(train_queries, train_passages, train_response, tokenizer, MAX_LENGTH)\n",
    "valid_encodings = add_start_and_end_positions(test_queries,test_passages, test_response, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_dataset = SQAD(train_encodings)\n",
    "valid_dataset = SQAD(valid_encodings)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=1)\n",
    "\n",
    "bert_base_model = AutoModelForQuestionAnswering.from_pretrained(MODEL_PATH)\n",
    "qa_model = QuestionAnsweringModel(bert_base_model, device)\n",
    "optimizer = AdamW(qa_model.parameters(), lr=2e-5, weight_decay=2e-2)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "MODEL_SAVE_PATH = \"qa_medium_model_scheduler.pt\"\n",
    "\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    qa_model = QuestionAnsweringModel(bert_base_model, device)\n",
    "    qa_model.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, 'model_weights.pt')))\n",
    "    qa_model.to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_PATH)\n",
    "    print(f\"Model loaded from {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    wer_metric = load(\"wer\")\n",
    "    avg_f1_score, wer_score  = qa_model.evaluate_model(valid_data_loader, tokenizer)\n",
    "\n",
    "    print(f\"WER Score: {wer_score}\")\n",
    "    print(f\"F1 Score: {avg_f1_score}\")\n",
    "else:\n",
    "    wer_metric = load(\"wer\")\n",
    "    EPOCHS = 6\n",
    "    wer_scores = []\n",
    "    train_accuracies = []\n",
    "    train_losses = []\n",
    "    f1_scores = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'Epoch - {epoch + 1}')\n",
    "        train_accuracy, train_loss = qa_model.train_epoch(train_data_loader, optimizer)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f\"Train Accuracy: {train_accuracy}      Train Loss: {train_loss}\")\n",
    "        avg_f1_score, wer_score  = qa_model.evaluate_model(valid_data_loader, tokenizer)\n",
    "        f1_scores.append(avg_f1_score)\n",
    "        print(f\"F1 Score: {avg_f1_score}\")\n",
    "        wer_scores.append(wer_score)\n",
    "    if not os.path.exists(MODEL_SAVE_PATH):\n",
    "        os.makedirs(MODEL_SAVE_PATH)\n",
    "    torch.save(qa_model.state_dict(), os.path.join(MODEL_SAVE_PATH, 'model_weights.pt'))\n",
    "    tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "    epochs = range(1, EPOCHS + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, train_losses, marker='o', linestyle='-', color='r')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, marker='o', linestyle='-', color='b')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, wer_scores, marker='o', linestyle='-', color='g')\n",
    "    plt.title('Word Error Rate (WER)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('WER')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, f1_scores, marker='o', linestyle='-', color='m')\n",
    "    plt.title('F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60ff912-941b-486c-9190-98127fbf035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from qa_medium_model_scheduler.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 17841/17841 [02:58<00:00, 99.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER Score: 4.155301020567398\n",
      "F1 Score: 0.3477184792208413\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from evaluate import load\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def extract_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    passage_list = []\n",
    "    query_list = []\n",
    "    response_list = []\n",
    "    for topic in data['data']:\n",
    "        for paragraph in topic['paragraphs']:\n",
    "            passage = paragraph['context'].lower()\n",
    "            for qa in paragraph['qas']:\n",
    "                query = qa['question'].lower()\n",
    "                for answer in qa['answers']:\n",
    "                    answer_text = answer['text'].lower()\n",
    "                    passage_list.append(passage)\n",
    "                    query_list.append(query)\n",
    "                    response_list.append({\n",
    "                        'text': answer_text,\n",
    "                        'answer_start': answer['answer_start'],\n",
    "                        'answer_end': answer['answer_start'] + len(answer_text)\n",
    "                    })\n",
    "    return passage_list, query_list, response_list\n",
    "\n",
    "class SQAD(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][i]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][i]),\n",
    "            'start_positions': torch.tensor(self.encodings['start_positions'][i]),\n",
    "            'end_positions': torch.tensor(self.encodings['end_positions'][i])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "def add_start_and_end_positions(queries, contexts, answers, tokenizer, max_length, doc_stride = 128):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    encodings = tokenizer(queries, contexts, max_length = MAX_LENGTH,truncation=True,padding=True, return_offsets_mapping=False, stride = doc_stride)\n",
    "    for idx, answer in enumerate(answers):\n",
    "        ret_start = 0\n",
    "        ret_end = 0\n",
    "        answer_encoding_fast = tokenizer(answer['text'], max_length=max_length, truncation=True, padding=True)\n",
    "        for a in range(len(encodings['input_ids'][idx]) - len(answer_encoding_fast['input_ids']) + 1):\n",
    "            match = True\n",
    "            for i in range(1, len(answer_encoding_fast['input_ids']) - 1):\n",
    "                if answer_encoding_fast['input_ids'][i] != encodings['input_ids'][idx][a + i]:\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:\n",
    "                ret_start = a + 1\n",
    "                ret_end = a + i + 1\n",
    "                break\n",
    "        start_positions.append(ret_start)\n",
    "        end_positions.append(ret_end)\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    return encodings\n",
    "\n",
    "def normalize_text(text):\n",
    "    def remove_articles(txt):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', txt)\n",
    "\n",
    "    def white_space_fix(txt):\n",
    "        return ' '.join(txt.split())\n",
    "\n",
    "    def remove_punc(txt):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in txt if ch not in exclude)\n",
    "\n",
    "    def lower(txt):\n",
    "        return txt.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(text))))\n",
    "\n",
    "class QuestionAnsweringModel(nn.Module):\n",
    "    def __init__(self, bert_base_model, device):\n",
    "        super(QuestionAnsweringModel, self).__init__()\n",
    "        self.bert = bert_base_model\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "        return start_logits, end_logits\n",
    "\n",
    "    def train_epoch(self, dataloader, optimizer, scheduler):\n",
    "        self.train()\n",
    "        loss_values = []\n",
    "        accuracy_values = []\n",
    "        for batch in tqdm(dataloader, desc='Training'):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            start_positions = batch['start_positions'].to(self.device)\n",
    "            end_positions = batch['end_positions'].to(self.device)\n",
    "            output_start, output_end = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = self.focal_loss(output_start, output_end, start_positions, end_positions, 1) #using gamma = 1\n",
    "            loss_values.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            start_predictions = torch.argmax(output_start, dim=1)\n",
    "            end_predictions = torch.argmax(output_end, dim=1)\n",
    "\n",
    "            accuracy_values.append(((start_predictions == start_positions).sum()/len(start_predictions)).item())\n",
    "            accuracy_values.append(((end_predictions == end_positions).sum()/len(end_predictions)).item())\n",
    "        scheduler.step()\n",
    "        return sum(accuracy_values)/len(accuracy_values), sum(loss_values)/len(loss_values)\n",
    "    \n",
    "    def focal_loss(self, start_logits, end_logits, start_positions, end_positions, gamma):\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        start_probs = softmax(start_logits)\n",
    "        inv_start_probs = 1 - start_probs\n",
    "        end_probs = softmax(end_logits)\n",
    "        inv_end_probs = 1 - end_probs\n",
    "        \n",
    "        log_softmax = nn.LogSoftmax(dim=1)\n",
    "        log_start_probs = log_softmax(start_logits)\n",
    "        log_end_probs = log_softmax(end_logits)\n",
    "        \n",
    "        negative_log_likelihood = nn.NLLLoss()\n",
    "        \n",
    "        start_loss = negative_log_likelihood(torch.pow(inv_start_probs, gamma) * log_start_probs, start_positions)\n",
    "        end_loss = negative_log_likelihood(torch.pow(inv_end_probs, gamma) * log_end_probs, end_positions)\n",
    "        \n",
    "        return (start_loss + end_loss) / 2\n",
    "    \n",
    "    def evaluate_model(self, dataloader, tokenizer):\n",
    "        self.eval()\n",
    "        f1_scores = []\n",
    "        answer_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                start_true_positions = batch['start_positions'].to(self.device)\n",
    "                end_true_positions = batch['end_positions'].to(self.device)\n",
    "\n",
    "                output_start, output_end = self(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                start_pred = torch.argmax(output_start, dim=1)\n",
    "                end_pred = torch.argmax(output_end, dim=1)\n",
    "\n",
    "                for i in range(input_ids.size(0)):\n",
    "                    pred_answer = tokenizer.decode(input_ids[i][start_pred[i]:end_pred[i]+1])\n",
    "                    true_answer = tokenizer.decode(input_ids[i][start_true_positions[i]:end_true_positions[i]+1])\n",
    "                    answer_list.append([pred_answer, true_answer])\n",
    "                    f1_scores.append(compute_f1_score(pred_answer, true_answer))\n",
    "        pred_answers = [ans[0] if ans[0] else \"$\" for ans in answer_list]\n",
    "        true_answers = [ans[1] if ans[1] else \"$\" for ans in answer_list]\n",
    "        wer_score = wer_metric.compute(predictions=pred_answers, references=true_answers)\n",
    "        avg_f1_score = sum(f1_scores) / len(f1_scores) if f1_scores else 0\n",
    "        return avg_f1_score, wer_score\n",
    "    \n",
    "def compute_f1_score(pred_answer, true_answer):\n",
    "    pred_tokens = normalize_text(pred_answer).split()\n",
    "    true_tokens = normalize_text(true_answer).split()\n",
    "    common_tokens = Counter(pred_tokens) & Counter(true_tokens)\n",
    "    num_common = sum(common_tokens.values())\n",
    "    if num_common == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_common / len(pred_tokens)\n",
    "    recall = 1.0 * num_common / len(true_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "train_data = 'spoken_train-v1.1.json'\n",
    "test_data = 'spoken_test-v1.1_WER54.json'\n",
    "train_passages, train_queries, train_response = extract_data(train_data)\n",
    "test_passages, test_queries, test_response = extract_data(test_data)\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "MODEL_PATH = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "train_encodings = add_start_and_end_positions(train_queries, train_passages, train_response, tokenizer, MAX_LENGTH)\n",
    "valid_encodings = add_start_and_end_positions(test_queries,test_passages, test_response, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_dataset = SQAD(train_encodings)\n",
    "valid_dataset = SQAD(valid_encodings)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=1)\n",
    "\n",
    "bert_base_model = AutoModelForQuestionAnswering.from_pretrained(MODEL_PATH)\n",
    "qa_model = QuestionAnsweringModel(bert_base_model, device)\n",
    "optimizer = AdamW(qa_model.parameters(), lr=2e-5, weight_decay=2e-2)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "MODEL_SAVE_PATH = \"qa_medium_model_scheduler.pt\"\n",
    "\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    qa_model = QuestionAnsweringModel(bert_base_model, device)\n",
    "    qa_model.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, 'model_weights.pt')))\n",
    "    qa_model.to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_PATH)\n",
    "    print(f\"Model loaded from {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    wer_metric = load(\"wer\")\n",
    "    avg_f1_score, wer_score  = qa_model.evaluate_model(valid_data_loader, tokenizer)\n",
    "\n",
    "    print(f\"WER Score: {wer_score}\")\n",
    "    print(f\"F1 Score: {avg_f1_score}\")\n",
    "else:\n",
    "    wer_metric = load(\"wer\")\n",
    "    EPOCHS = 6\n",
    "    wer_scores = []\n",
    "    train_accuracies = []\n",
    "    train_losses = []\n",
    "    f1_scores = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'Epoch - {epoch + 1}')\n",
    "        train_accuracy, train_loss = qa_model.train_epoch(train_data_loader, optimizer)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f\"Train Accuracy: {train_accuracy}      Train Loss: {train_loss}\")\n",
    "        avg_f1_score, wer_score  = qa_model.evaluate_model(valid_data_loader, tokenizer)\n",
    "        f1_scores.append(avg_f1_score)\n",
    "        print(f\"F1 Score: {avg_f1_score}\")\n",
    "        wer_scores.append(wer_score)\n",
    "    if not os.path.exists(MODEL_SAVE_PATH):\n",
    "        os.makedirs(MODEL_SAVE_PATH)\n",
    "    torch.save(qa_model.state_dict(), os.path.join(MODEL_SAVE_PATH, 'model_weights.pt'))\n",
    "    tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "    epochs = range(1, EPOCHS + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, train_losses, marker='o', linestyle='-', color='r')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, marker='o', linestyle='-', color='b')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, wer_scores, marker='o', linestyle='-', color='g')\n",
    "    plt.title('Word Error Rate (WER)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('WER')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, f1_scores, marker='o', linestyle='-', color='m')\n",
    "    plt.title('F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
